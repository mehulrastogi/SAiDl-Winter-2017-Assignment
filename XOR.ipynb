{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FC_NN_layer_foward(X,W,b):\n",
    "    Z =  X.dot(W) + b\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def act_relu_foward(Z):\n",
    "#     print(Z)\n",
    "    a = np.maximum(0, Z)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def declare(l0 , l1):\n",
    "    W = np.random.randn(l0 ,l1) * np.sqrt(2.0 /l0 )\n",
    "    b = np.zeros((l1))\n",
    "    return W,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def dec_dic(layers_size):\n",
    "    params = {}\n",
    "    for i in range(1 , layers_size.shape[0]):\n",
    "            params['W' + str(i)] , params['b' + str(i)] = declare(layers_size[i-1] , layers_size[i])\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1 / (1 + np.exp(-z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Declaring Important Variables ############\n",
    "#############################################\n",
    "X_train = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "Y_train = np.array([[1],[0],[0],[1]])\n",
    "\n",
    "# (2 - inputs) - \n",
    "layers_size = np.array([2 , 3,1])  #feature dim of each layer\n",
    "params = dec_dic(layers_size)#creating a dictionary with all the parameters\n",
    "#     params\n",
    "\n",
    "m = X_train.shape[0]\n",
    "grads ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propogation(X ,Y,layers_size,reg):\n",
    "    \n",
    "    Z1 = FC_NN_layer_foward(X,params['W1'],params['b1'])\n",
    "    A1 = act_relu_foward(Z1)\n",
    "    Z2 = FC_NN_layer_foward(A1,params['W2'],params['b2'])\n",
    "    \n",
    "    score = Z2\n",
    "    \n",
    "#     calculating loss\n",
    "    normal = sigmoid(score)#(n,1)\n",
    "    \n",
    "#     print(normal.shape)\n",
    "    \n",
    "    loss = (Y*np.log(normal))+((1-Y)*np.log(1-normal))\n",
    "    loss = -1 * np.sum(loss , axis = 0) / m\n",
    "#     loss += 0.5 * reg * (np.sum(params['W1'] * params['W1']) +np.sum(params['W2'] * params['W2']))\n",
    "    \n",
    "    \n",
    "    cache = (Z1 , A1 , normal , X,Y,reg)\n",
    "    return loss , cache,normal\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_propogation(cache):\n",
    "    \n",
    "    W1,W2,b1,b2 = params['W1'] , params['W2'] , params['b1'] , params['b2']\n",
    "    Z1,A1,normal,X,Y,reg  = cache\n",
    "#     print(Y.shape)\n",
    "    dloss = normal\n",
    "#     print(dloss)\n",
    "    \n",
    "    dloss[Y==1] -= 1\n",
    "#     print(dloss)\n",
    "    dW2 = A1.T.dot(dloss)\n",
    "    db2 = np.sum(dloss , axis =0)\n",
    "#     dW2 += reg*W2\n",
    "    \n",
    "    dA1 = dloss.dot(W2.T)\n",
    "    dZ1 = dA1\n",
    "    dZ1[A1 < 0] = 0 \n",
    "    dW1 = X.T.dot(dZ1)\n",
    "    db1 = np.sum(dZ1 , axis =0)\n",
    "#     dW1 += reg*W1\n",
    "    \n",
    "#     UPDATE\n",
    "    grads['W1'] = dW1\n",
    "    grads['W2'] = dW2\n",
    "    grads['b1'] = db1\n",
    "    grads['b2'] = db2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(learning_rate):\n",
    "    W1,W2,b1,b2 = params['W1'] , params['W2'] , params['b1'] , params['b2']\n",
    "    dW1,dW2,db1,db2 = grads['W1'] , grads['W2'] , grads['b1'] , grads['b2']\n",
    "#     applying sgd\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b1 -= learning_rate * db1\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    W1,W2,b1,b2 = params['W1'] , params['W2'] , params['b1'] , params['b2']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss , cache,normal = forward_propogation(X_train ,Y_train,layers_size,0.4)\n",
    "# normal.shape\n",
    "back_propogation(cache)\n",
    "update(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "The accurracy in iteration 0 is 0.500000\n",
      "[[ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]]\n",
      "The accurracy in iteration 1 is 0.500000\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "The accurracy in iteration 2 is 0.500000\n",
      "[[ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]]\n",
      "The accurracy in iteration 3 is 0.500000\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "The accurracy in iteration 4 is 0.500000\n",
      "[[ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]]\n",
      "The accurracy in iteration 5 is 0.500000\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "The accurracy in iteration 6 is 0.500000\n",
      "[[ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]]\n",
      "The accurracy in iteration 7 is 0.500000\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "The accurracy in iteration 8 is 0.500000\n",
      "[[ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]]\n",
      "The accurracy in iteration 9 is 0.500000\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "The accurracy in iteration 10 is 0.500000\n",
      "[[ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]]\n",
      "The accurracy in iteration 11 is 0.500000\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "The accurracy in iteration 12 is 0.500000\n",
      "[[ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]]\n",
      "The accurracy in iteration 13 is 0.500000\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "The accurracy in iteration 14 is 0.500000\n",
      "[[ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]]\n",
      "The accurracy in iteration 15 is 0.500000\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "The accurracy in iteration 16 is 0.500000\n",
      "[[ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]]\n",
      "The accurracy in iteration 17 is 0.500000\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n",
      "The accurracy in iteration 18 is 0.500000\n",
      "[[ True]\n",
      " [False]\n",
      " [False]\n",
      " [ True]]\n",
      "The accurracy in iteration 19 is 0.500000\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    loss,cache,pre = forward_propogation(X_train,Y_train,layers_size,0.4)\n",
    "#     print(pre)\n",
    "    pre[pre>0.5] = 1\n",
    "    pre[pre<= 0.5] = 0\n",
    "#     pre.reshape((1,4))\n",
    "    print(pre)\n",
    "    print(pre==Y_train)\n",
    "    acc = np.sum(pre==Y_train)\n",
    "    print(\"The accurracy in iteration %i is %f\" %(i,acc/4))\n",
    "    back_propogation(cache)\n",
    "    update(0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "# printing predicted result\n",
    "print(pre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
